{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UhAk8jMsQn3w",
        "outputId": "b6f2fe62-ed44-4701-c96b-f560a2628edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import GraphConv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NODECLASSIFICATION\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "id": "f6NGOU2YRoad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221ab3bd-165a-4c5a-8b71-61dd43523fad"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: CiteSeer():\n",
            "====================\n",
            "Number of graphs: 1\n",
            "Number of features: 3703\n",
            "Number of classes: 6\n",
            "\n",
            "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
            "=============================================================\n",
            "Number of nodes: 3327\n",
            "Number of edges: 9104\n",
            "Average node degree: 2.74\n",
            "Has isolated nodes: True\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NODECLASSIFICATION\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "2YgIQ19SQpGm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN().to(device)\n",
        "print(model)\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "learning_rates = [0.1,0.01,0.001,0.0001]\n",
        "weight_decays = [0.1,0.01,0.001,0.0001]\n",
        "for learning_rate in learning_rates:\n",
        "  for weight_decay in weight_decays:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    model.train()\n",
        "    print(f'Learning_rate: {learning_rate}, weight_decay: {weight_decay}')\n",
        "    print('=============================================================')\n",
        "    print()\n",
        "    for epoch in range(1, 12):\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data)\n",
        "      loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      model.eval()\n",
        "      pred = model(data).argmax(dim=1)\n",
        "      correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "      test_acc = int(correct) / int(data.test_mask.sum())\n",
        "      print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "bDqRPNZiQzQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ea9981-2e42-455c-e7a4-dec7164d2877"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(3703, 16)\n",
            "  (conv2): GCNConv(16, 6)\n",
            ")\n",
            "Learning_rate: 0.1, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.2970\n",
            "Epoch: 002, Test Acc: 0.6290\n",
            "Epoch: 003, Test Acc: 0.6040\n",
            "Epoch: 004, Test Acc: 0.5380\n",
            "Epoch: 005, Test Acc: 0.5880\n",
            "Epoch: 006, Test Acc: 0.6730\n",
            "Epoch: 007, Test Acc: 0.7250\n",
            "Epoch: 008, Test Acc: 0.7000\n",
            "Epoch: 009, Test Acc: 0.6850\n",
            "Epoch: 010, Test Acc: 0.6880\n",
            "Epoch: 011, Test Acc: 0.6770\n",
            "Learning_rate: 0.1, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5780\n",
            "Epoch: 002, Test Acc: 0.6080\n",
            "Epoch: 003, Test Acc: 0.6540\n",
            "Epoch: 004, Test Acc: 0.6010\n",
            "Epoch: 005, Test Acc: 0.6720\n",
            "Epoch: 006, Test Acc: 0.6400\n",
            "Epoch: 007, Test Acc: 0.6850\n",
            "Epoch: 008, Test Acc: 0.6910\n",
            "Epoch: 009, Test Acc: 0.6830\n",
            "Epoch: 010, Test Acc: 0.6850\n",
            "Epoch: 011, Test Acc: 0.6910\n",
            "Learning_rate: 0.1, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.4820\n",
            "Epoch: 002, Test Acc: 0.3670\n",
            "Epoch: 003, Test Acc: 0.4600\n",
            "Epoch: 004, Test Acc: 0.4720\n",
            "Epoch: 005, Test Acc: 0.4790\n",
            "Epoch: 006, Test Acc: 0.4800\n",
            "Epoch: 007, Test Acc: 0.4920\n",
            "Epoch: 008, Test Acc: 0.5230\n",
            "Epoch: 009, Test Acc: 0.5440\n",
            "Epoch: 010, Test Acc: 0.5570\n",
            "Epoch: 011, Test Acc: 0.5740\n",
            "Learning_rate: 0.1, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.4650\n",
            "Epoch: 002, Test Acc: 0.5970\n",
            "Epoch: 003, Test Acc: 0.5910\n",
            "Epoch: 004, Test Acc: 0.6230\n",
            "Epoch: 005, Test Acc: 0.5950\n",
            "Epoch: 006, Test Acc: 0.5830\n",
            "Epoch: 007, Test Acc: 0.5730\n",
            "Epoch: 008, Test Acc: 0.5610\n",
            "Epoch: 009, Test Acc: 0.5530\n",
            "Epoch: 010, Test Acc: 0.5620\n",
            "Epoch: 011, Test Acc: 0.5550\n",
            "Learning_rate: 0.01, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5570\n",
            "Epoch: 002, Test Acc: 0.5640\n",
            "Epoch: 003, Test Acc: 0.5650\n",
            "Epoch: 004, Test Acc: 0.5650\n",
            "Epoch: 005, Test Acc: 0.5650\n",
            "Epoch: 006, Test Acc: 0.5600\n",
            "Epoch: 007, Test Acc: 0.5570\n",
            "Epoch: 008, Test Acc: 0.5550\n",
            "Epoch: 009, Test Acc: 0.5510\n",
            "Epoch: 010, Test Acc: 0.5490\n",
            "Epoch: 011, Test Acc: 0.5450\n",
            "Learning_rate: 0.01, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5750\n",
            "Epoch: 002, Test Acc: 0.5860\n",
            "Epoch: 003, Test Acc: 0.5880\n",
            "Epoch: 004, Test Acc: 0.5920\n",
            "Epoch: 005, Test Acc: 0.5980\n",
            "Epoch: 006, Test Acc: 0.6030\n",
            "Epoch: 007, Test Acc: 0.6050\n",
            "Epoch: 008, Test Acc: 0.6080\n",
            "Epoch: 009, Test Acc: 0.6080\n",
            "Epoch: 010, Test Acc: 0.6140\n",
            "Epoch: 011, Test Acc: 0.6140\n",
            "Learning_rate: 0.01, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6130\n",
            "Epoch: 002, Test Acc: 0.6240\n",
            "Epoch: 003, Test Acc: 0.6340\n",
            "Epoch: 004, Test Acc: 0.6410\n",
            "Epoch: 005, Test Acc: 0.6450\n",
            "Epoch: 006, Test Acc: 0.6490\n",
            "Epoch: 007, Test Acc: 0.6520\n",
            "Epoch: 008, Test Acc: 0.6590\n",
            "Epoch: 009, Test Acc: 0.6620\n",
            "Epoch: 010, Test Acc: 0.6610\n",
            "Epoch: 011, Test Acc: 0.6590\n",
            "Learning_rate: 0.01, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6530\n",
            "Epoch: 002, Test Acc: 0.6540\n",
            "Epoch: 003, Test Acc: 0.6550\n",
            "Epoch: 004, Test Acc: 0.6560\n",
            "Epoch: 005, Test Acc: 0.6600\n",
            "Epoch: 006, Test Acc: 0.6620\n",
            "Epoch: 007, Test Acc: 0.6610\n",
            "Epoch: 008, Test Acc: 0.6630\n",
            "Epoch: 009, Test Acc: 0.6630\n",
            "Epoch: 010, Test Acc: 0.6660\n",
            "Epoch: 011, Test Acc: 0.6670\n",
            "Learning_rate: 0.001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6650\n",
            "Epoch: 002, Test Acc: 0.6650\n",
            "Epoch: 003, Test Acc: 0.6650\n",
            "Epoch: 004, Test Acc: 0.6660\n",
            "Epoch: 005, Test Acc: 0.6650\n",
            "Epoch: 006, Test Acc: 0.6650\n",
            "Epoch: 007, Test Acc: 0.6660\n",
            "Epoch: 008, Test Acc: 0.6650\n",
            "Epoch: 009, Test Acc: 0.6650\n",
            "Epoch: 010, Test Acc: 0.6660\n",
            "Epoch: 011, Test Acc: 0.6660\n",
            "Learning_rate: 0.001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6650\n",
            "Epoch: 002, Test Acc: 0.6640\n",
            "Epoch: 003, Test Acc: 0.6650\n",
            "Epoch: 004, Test Acc: 0.6640\n",
            "Epoch: 005, Test Acc: 0.6640\n",
            "Epoch: 006, Test Acc: 0.6650\n",
            "Epoch: 007, Test Acc: 0.6650\n",
            "Epoch: 008, Test Acc: 0.6640\n",
            "Epoch: 009, Test Acc: 0.6630\n",
            "Epoch: 010, Test Acc: 0.6630\n",
            "Epoch: 011, Test Acc: 0.6640\n",
            "Learning_rate: 0.001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6690\n",
            "Epoch: 002, Test Acc: 0.6700\n",
            "Epoch: 003, Test Acc: 0.6700\n",
            "Epoch: 004, Test Acc: 0.6720\n",
            "Epoch: 005, Test Acc: 0.6710\n",
            "Epoch: 006, Test Acc: 0.6740\n",
            "Epoch: 007, Test Acc: 0.6750\n",
            "Epoch: 008, Test Acc: 0.6730\n",
            "Epoch: 009, Test Acc: 0.6720\n",
            "Epoch: 010, Test Acc: 0.6720\n",
            "Epoch: 011, Test Acc: 0.6720\n",
            "Learning_rate: 0.001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6720\n",
            "Epoch: 002, Test Acc: 0.6730\n",
            "Epoch: 003, Test Acc: 0.6710\n",
            "Epoch: 004, Test Acc: 0.6740\n",
            "Epoch: 005, Test Acc: 0.6740\n",
            "Epoch: 006, Test Acc: 0.6740\n",
            "Epoch: 007, Test Acc: 0.6740\n",
            "Epoch: 008, Test Acc: 0.6750\n",
            "Epoch: 009, Test Acc: 0.6730\n",
            "Epoch: 010, Test Acc: 0.6740\n",
            "Epoch: 011, Test Acc: 0.6760\n",
            "Learning_rate: 0.0001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6770\n",
            "Epoch: 002, Test Acc: 0.6770\n",
            "Epoch: 003, Test Acc: 0.6770\n",
            "Epoch: 004, Test Acc: 0.6770\n",
            "Epoch: 005, Test Acc: 0.6760\n",
            "Epoch: 006, Test Acc: 0.6770\n",
            "Epoch: 007, Test Acc: 0.6760\n",
            "Epoch: 008, Test Acc: 0.6760\n",
            "Epoch: 009, Test Acc: 0.6770\n",
            "Epoch: 010, Test Acc: 0.6770\n",
            "Epoch: 011, Test Acc: 0.6770\n",
            "Learning_rate: 0.0001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6770\n",
            "Epoch: 002, Test Acc: 0.6770\n",
            "Epoch: 003, Test Acc: 0.6780\n",
            "Epoch: 004, Test Acc: 0.6780\n",
            "Epoch: 005, Test Acc: 0.6780\n",
            "Epoch: 006, Test Acc: 0.6780\n",
            "Epoch: 007, Test Acc: 0.6780\n",
            "Epoch: 008, Test Acc: 0.6780\n",
            "Epoch: 009, Test Acc: 0.6780\n",
            "Epoch: 010, Test Acc: 0.6780\n",
            "Epoch: 011, Test Acc: 0.6780\n",
            "Learning_rate: 0.0001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6780\n",
            "Epoch: 002, Test Acc: 0.6770\n",
            "Epoch: 003, Test Acc: 0.6760\n",
            "Epoch: 004, Test Acc: 0.6760\n",
            "Epoch: 005, Test Acc: 0.6760\n",
            "Epoch: 006, Test Acc: 0.6760\n",
            "Epoch: 007, Test Acc: 0.6760\n",
            "Epoch: 008, Test Acc: 0.6760\n",
            "Epoch: 009, Test Acc: 0.6760\n",
            "Epoch: 010, Test Acc: 0.6760\n",
            "Epoch: 011, Test Acc: 0.6760\n",
            "Learning_rate: 0.0001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6740\n",
            "Epoch: 002, Test Acc: 0.6750\n",
            "Epoch: 003, Test Acc: 0.6760\n",
            "Epoch: 004, Test Acc: 0.6770\n",
            "Epoch: 005, Test Acc: 0.6770\n",
            "Epoch: 006, Test Acc: 0.6770\n",
            "Epoch: 007, Test Acc: 0.6770\n",
            "Epoch: 008, Test Acc: 0.6770\n",
            "Epoch: 009, Test Acc: 0.6770\n",
            "Epoch: 010, Test Acc: 0.6780\n",
            "Epoch: 011, Test Acc: 0.6780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, 16)\n",
        "        self.conv2 = GraphConv(16, dataset.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "l9ArjyBkLyst"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN().to(device)\n",
        "print(model)\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "learning_rates = [0.1,0.01,0.001,0.0001]\n",
        "weight_decays = [0.1,0.01,0.001,0.0001]\n",
        "for learning_rate in learning_rates:\n",
        "  for weight_decay in weight_decays:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    model.train()\n",
        "    print(f'Learning_rate: {learning_rate}, weight_decay: {weight_decay}')\n",
        "    print('=============================================================')\n",
        "    print()\n",
        "    for epoch in range(1, 12):\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data)\n",
        "      loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      model.eval()\n",
        "      pred = model(data).argmax(dim=1)\n",
        "      correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "      test_acc = int(correct) / int(data.test_mask.sum())\n",
        "      print(f'Epoch: {epoch:03d}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "USVpgi9bQtxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b65dab9-864f-4f1d-b243-1cf6e4f02787"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GraphConv(3703, 16)\n",
            "  (conv2): GraphConv(16, 6)\n",
            ")\n",
            "Learning_rate: 0.1, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.2420\n",
            "Epoch: 002, Test Acc: 0.1880\n",
            "Epoch: 003, Test Acc: 0.3040\n",
            "Epoch: 004, Test Acc: 0.2310\n",
            "Epoch: 005, Test Acc: 0.3400\n",
            "Epoch: 006, Test Acc: 0.4550\n",
            "Epoch: 007, Test Acc: 0.4200\n",
            "Epoch: 008, Test Acc: 0.4470\n",
            "Epoch: 009, Test Acc: 0.4520\n",
            "Epoch: 010, Test Acc: 0.4680\n",
            "Epoch: 011, Test Acc: 0.4440\n",
            "Learning_rate: 0.1, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.3190\n",
            "Epoch: 002, Test Acc: 0.3670\n",
            "Epoch: 003, Test Acc: 0.2580\n",
            "Epoch: 004, Test Acc: 0.3290\n",
            "Epoch: 005, Test Acc: 0.3360\n",
            "Epoch: 006, Test Acc: 0.3560\n",
            "Epoch: 007, Test Acc: 0.3590\n",
            "Epoch: 008, Test Acc: 0.3490\n",
            "Epoch: 009, Test Acc: 0.3550\n",
            "Epoch: 010, Test Acc: 0.3820\n",
            "Epoch: 011, Test Acc: 0.4020\n",
            "Learning_rate: 0.1, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.4050\n",
            "Epoch: 002, Test Acc: 0.4400\n",
            "Epoch: 003, Test Acc: 0.3450\n",
            "Epoch: 004, Test Acc: 0.4150\n",
            "Epoch: 005, Test Acc: 0.4790\n",
            "Epoch: 006, Test Acc: 0.4690\n",
            "Epoch: 007, Test Acc: 0.4330\n",
            "Epoch: 008, Test Acc: 0.3980\n",
            "Epoch: 009, Test Acc: 0.3880\n",
            "Epoch: 010, Test Acc: 0.3800\n",
            "Epoch: 011, Test Acc: 0.3760\n",
            "Learning_rate: 0.1, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.4580\n",
            "Epoch: 002, Test Acc: 0.4700\n",
            "Epoch: 003, Test Acc: 0.4330\n",
            "Epoch: 004, Test Acc: 0.3370\n",
            "Epoch: 005, Test Acc: 0.4170\n",
            "Epoch: 006, Test Acc: 0.5190\n",
            "Epoch: 007, Test Acc: 0.5130\n",
            "Epoch: 008, Test Acc: 0.5290\n",
            "Epoch: 009, Test Acc: 0.5330\n",
            "Epoch: 010, Test Acc: 0.5310\n",
            "Epoch: 011, Test Acc: 0.5020\n",
            "Learning_rate: 0.01, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5100\n",
            "Epoch: 002, Test Acc: 0.5150\n",
            "Epoch: 003, Test Acc: 0.5200\n",
            "Epoch: 004, Test Acc: 0.5240\n",
            "Epoch: 005, Test Acc: 0.5290\n",
            "Epoch: 006, Test Acc: 0.5340\n",
            "Epoch: 007, Test Acc: 0.5370\n",
            "Epoch: 008, Test Acc: 0.5400\n",
            "Epoch: 009, Test Acc: 0.5400\n",
            "Epoch: 010, Test Acc: 0.5410\n",
            "Epoch: 011, Test Acc: 0.5360\n",
            "Learning_rate: 0.01, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5370\n",
            "Epoch: 002, Test Acc: 0.5400\n",
            "Epoch: 003, Test Acc: 0.5440\n",
            "Epoch: 004, Test Acc: 0.5450\n",
            "Epoch: 005, Test Acc: 0.5480\n",
            "Epoch: 006, Test Acc: 0.5480\n",
            "Epoch: 007, Test Acc: 0.5490\n",
            "Epoch: 008, Test Acc: 0.5480\n",
            "Epoch: 009, Test Acc: 0.5470\n",
            "Epoch: 010, Test Acc: 0.5430\n",
            "Epoch: 011, Test Acc: 0.5450\n",
            "Learning_rate: 0.01, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5580\n",
            "Epoch: 002, Test Acc: 0.5700\n",
            "Epoch: 003, Test Acc: 0.5720\n",
            "Epoch: 004, Test Acc: 0.5640\n",
            "Epoch: 005, Test Acc: 0.5600\n",
            "Epoch: 006, Test Acc: 0.5590\n",
            "Epoch: 007, Test Acc: 0.5570\n",
            "Epoch: 008, Test Acc: 0.5530\n",
            "Epoch: 009, Test Acc: 0.5480\n",
            "Epoch: 010, Test Acc: 0.5500\n",
            "Epoch: 011, Test Acc: 0.5420\n",
            "Learning_rate: 0.01, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.5750\n",
            "Epoch: 002, Test Acc: 0.5980\n",
            "Epoch: 003, Test Acc: 0.6080\n",
            "Epoch: 004, Test Acc: 0.6100\n",
            "Epoch: 005, Test Acc: 0.6160\n",
            "Epoch: 006, Test Acc: 0.6250\n",
            "Epoch: 007, Test Acc: 0.6260\n",
            "Epoch: 008, Test Acc: 0.6260\n",
            "Epoch: 009, Test Acc: 0.6320\n",
            "Epoch: 010, Test Acc: 0.6320\n",
            "Epoch: 011, Test Acc: 0.6330\n",
            "Learning_rate: 0.001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6280\n",
            "Epoch: 002, Test Acc: 0.6290\n",
            "Epoch: 003, Test Acc: 0.6290\n",
            "Epoch: 004, Test Acc: 0.6290\n",
            "Epoch: 005, Test Acc: 0.6290\n",
            "Epoch: 006, Test Acc: 0.6280\n",
            "Epoch: 007, Test Acc: 0.6280\n",
            "Epoch: 008, Test Acc: 0.6280\n",
            "Epoch: 009, Test Acc: 0.6290\n",
            "Epoch: 010, Test Acc: 0.6300\n",
            "Epoch: 011, Test Acc: 0.6300\n",
            "Learning_rate: 0.001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6290\n",
            "Epoch: 002, Test Acc: 0.6280\n",
            "Epoch: 003, Test Acc: 0.6280\n",
            "Epoch: 004, Test Acc: 0.6280\n",
            "Epoch: 005, Test Acc: 0.6280\n",
            "Epoch: 006, Test Acc: 0.6280\n",
            "Epoch: 007, Test Acc: 0.6270\n",
            "Epoch: 008, Test Acc: 0.6270\n",
            "Epoch: 009, Test Acc: 0.6270\n",
            "Epoch: 010, Test Acc: 0.6270\n",
            "Epoch: 011, Test Acc: 0.6270\n",
            "Learning_rate: 0.001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6260\n",
            "Epoch: 002, Test Acc: 0.6260\n",
            "Epoch: 003, Test Acc: 0.6250\n",
            "Epoch: 004, Test Acc: 0.6250\n",
            "Epoch: 005, Test Acc: 0.6280\n",
            "Epoch: 006, Test Acc: 0.6280\n",
            "Epoch: 007, Test Acc: 0.6310\n",
            "Epoch: 008, Test Acc: 0.6320\n",
            "Epoch: 009, Test Acc: 0.6310\n",
            "Epoch: 010, Test Acc: 0.6310\n",
            "Epoch: 011, Test Acc: 0.6320\n",
            "Learning_rate: 0.001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6310\n",
            "Epoch: 002, Test Acc: 0.6310\n",
            "Epoch: 003, Test Acc: 0.6310\n",
            "Epoch: 004, Test Acc: 0.6310\n",
            "Epoch: 005, Test Acc: 0.6300\n",
            "Epoch: 006, Test Acc: 0.6300\n",
            "Epoch: 007, Test Acc: 0.6300\n",
            "Epoch: 008, Test Acc: 0.6310\n",
            "Epoch: 009, Test Acc: 0.6320\n",
            "Epoch: 010, Test Acc: 0.6330\n",
            "Epoch: 011, Test Acc: 0.6320\n",
            "Learning_rate: 0.0001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6320\n",
            "Epoch: 002, Test Acc: 0.6320\n",
            "Epoch: 003, Test Acc: 0.6320\n",
            "Epoch: 004, Test Acc: 0.6320\n",
            "Epoch: 005, Test Acc: 0.6320\n",
            "Epoch: 006, Test Acc: 0.6320\n",
            "Epoch: 007, Test Acc: 0.6310\n",
            "Epoch: 008, Test Acc: 0.6310\n",
            "Epoch: 009, Test Acc: 0.6310\n",
            "Epoch: 010, Test Acc: 0.6310\n",
            "Epoch: 011, Test Acc: 0.6310\n",
            "Learning_rate: 0.0001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6310\n",
            "Epoch: 002, Test Acc: 0.6310\n",
            "Epoch: 003, Test Acc: 0.6310\n",
            "Epoch: 004, Test Acc: 0.6300\n",
            "Epoch: 005, Test Acc: 0.6300\n",
            "Epoch: 006, Test Acc: 0.6300\n",
            "Epoch: 007, Test Acc: 0.6300\n",
            "Epoch: 008, Test Acc: 0.6300\n",
            "Epoch: 009, Test Acc: 0.6290\n",
            "Epoch: 010, Test Acc: 0.6290\n",
            "Epoch: 011, Test Acc: 0.6290\n",
            "Learning_rate: 0.0001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6290\n",
            "Epoch: 002, Test Acc: 0.6290\n",
            "Epoch: 003, Test Acc: 0.6290\n",
            "Epoch: 004, Test Acc: 0.6290\n",
            "Epoch: 005, Test Acc: 0.6290\n",
            "Epoch: 006, Test Acc: 0.6290\n",
            "Epoch: 007, Test Acc: 0.6290\n",
            "Epoch: 008, Test Acc: 0.6290\n",
            "Epoch: 009, Test Acc: 0.6290\n",
            "Epoch: 010, Test Acc: 0.6290\n",
            "Epoch: 011, Test Acc: 0.6290\n",
            "Learning_rate: 0.0001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Test Acc: 0.6290\n",
            "Epoch: 002, Test Acc: 0.6290\n",
            "Epoch: 003, Test Acc: 0.6300\n",
            "Epoch: 004, Test Acc: 0.6300\n",
            "Epoch: 005, Test Acc: 0.6320\n",
            "Epoch: 006, Test Acc: 0.6320\n",
            "Epoch: 007, Test Acc: 0.6330\n",
            "Epoch: 008, Test Acc: 0.6340\n",
            "Epoch: 009, Test Acc: 0.6340\n",
            "Epoch: 010, Test Acc: 0.6340\n",
            "Epoch: 011, Test Acc: 0.6340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GRAPHCLASSIFICATION\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faUBRfiU8SIN",
        "outputId": "8a37bdfe-ca8f-449a-ed62-68598f2df7a2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5niLtucS8bIQ",
        "outputId": "ee5becc7-7dee-4a2e-e975-89923d1f69bc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTqUbw1t8mki",
        "outputId": "e7c4baae-40c8-4cdf-c88f-ea9d1ff332e9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2506], x=[1139, 7], edge_attr=[2506, 4], y=[64], batch=[1139], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "DataBatch(edge_index=[2, 852], x=[387, 7], edge_attr=[852, 4], y=[22], batch=[387], ptr=[23])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdAFaesF8qVi",
        "outputId": "0e47b246-4b6e-4a63-a779-cb9acf258cf1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         loss = criterion(out, data.y)\n",
        "         loss.backward()\n",
        "         optimizer.step()\n",
        "         optimizer.zero_grad()\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:\n",
        "         out = model(data.x, data.edge_index, data.batch)\n",
        "         pred = out.argmax(dim=1)\n",
        "         correct += int((pred == data.y).sum())\n",
        "     return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "xQ812Zmi8ta_"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1,0.01,0.001,0.0001]\n",
        "weight_decays = [0.1,0.01,0.001,0.0001]\n",
        "for learning_rate in learning_rates:\n",
        "  for weight_decay in weight_decays:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    print(f'Learning_rate: {learning_rate}, weight_decay: {weight_decay}')\n",
        "    print('=============================================================')\n",
        "    print()\n",
        "    for epoch in range(1, 12):\n",
        "      train()\n",
        "      train_acc = test(train_loader)\n",
        "      test_acc = test(test_loader)\n",
        "      print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "Bh0nNQcbLSDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219b52a4-5257-4a8d-a5d1-bd4248c3d52a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate: 0.1, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6933, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.6733, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 011, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7067, Test Acc: 0.7632\n",
            "Epoch: 004, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 005, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 007, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 010, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 011, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 002, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 003, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 004, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 011, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 004, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = GNN(hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNdsJgDs8zL2",
        "outputId": "c36cf88a-b9df-4562-be12-e5275d548c5a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GNN(hidden_channels=64)\n",
        "print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCj6a6HU9IVo",
        "outputId": "b689d491-b468-4059-b18f-81ce5dba2345"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.1,0.01,0.001,0.0001]\n",
        "weight_decays = [0.1,0.01,0.001,0.0001]\n",
        "for learning_rate in learning_rates:\n",
        "  for weight_decay in weight_decays:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
        "    print(f'Learning_rate: {learning_rate}, weight_decay: {weight_decay}')\n",
        "    print('=============================================================')\n",
        "    print()\n",
        "    for epoch in range(1, 12):\n",
        "      train()\n",
        "      train_acc = test(train_loader)\n",
        "      test_acc = test(test_loader)\n",
        "      print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "    print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "ZULAAr2F9K0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b8166c-0c3f-4782-a85c-00495ef10899"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning_rate: 0.1, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 003, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 003, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 004, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6933, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.7733, Test Acc: 0.6842\n",
            "Epoch: 010, Train Acc: 0.7000, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7368\n",
            "\n",
            "\n",
            "Learning_rate: 0.1, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 003, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 004, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 005, Train Acc: 0.6600, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 011, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7333, Test Acc: 0.8421\n",
            "Epoch: 002, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 004, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 006, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 007, Train Acc: 0.6933, Test Acc: 0.7632\n",
            "Epoch: 008, Train Acc: 0.6933, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 002, Train Acc: 0.7400, Test Acc: 0.8421\n",
            "Epoch: 003, Train Acc: 0.7400, Test Acc: 0.8421\n",
            "Epoch: 004, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 005, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 006, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 007, Train Acc: 0.7400, Test Acc: 0.8421\n",
            "Epoch: 008, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 010, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 011, Train Acc: 0.7600, Test Acc: 0.8421\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.5800, Test Acc: 0.4474\n",
            "Epoch: 002, Train Acc: 0.7467, Test Acc: 0.8158\n",
            "Epoch: 003, Train Acc: 0.7400, Test Acc: 0.8158\n",
            "Epoch: 004, Train Acc: 0.7800, Test Acc: 0.8421\n",
            "Epoch: 005, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 006, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.7867, Test Acc: 0.8421\n",
            "Epoch: 009, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 010, Train Acc: 0.7467, Test Acc: 0.8158\n",
            "Epoch: 011, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "\n",
            "\n",
            "Learning_rate: 0.01, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 002, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 003, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 004, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 007, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.7933, Test Acc: 0.8421\n",
            "Epoch: 010, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 011, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 003, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 004, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 007, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.8133, Test Acc: 0.8158\n",
            "Epoch: 011, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 005, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 011, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.8000, Test Acc: 0.8421\n",
            "Epoch: 002, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.7933, Test Acc: 0.8158\n",
            "Epoch: 007, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.8000, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.8067, Test Acc: 0.8421\n",
            "Epoch: 003, Train Acc: 0.8067, Test Acc: 0.8421\n",
            "Epoch: 004, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 009, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 010, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.8267, Test Acc: 0.8158\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.1\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.8267, Test Acc: 0.8158\n",
            "Epoch: 002, Train Acc: 0.8200, Test Acc: 0.8158\n",
            "Epoch: 003, Train Acc: 0.8000, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8067, Test Acc: 0.8158\n",
            "Epoch: 008, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.01\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "\n",
            "\n",
            "Learning_rate: 0.0001, weight_decay: 0.0001\n",
            "=============================================================\n",
            "\n",
            "Epoch: 001, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 002, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 003, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 004, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 005, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 006, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 007, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 008, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 009, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 010, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.8067, Test Acc: 0.7895\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xN-kpBRON3YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}